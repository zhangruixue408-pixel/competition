# edge_douban_crawler.py
"""
å®Œæ•´çš„Edgeæµè§ˆå™¨è±†ç“£ä¹¦ç±çˆ¬è™«
"""
import os
import time
import random
import re
import json
from urllib.parse import quote
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pymysql
import requests

# å¯¼å…¥ä½ çš„æ•°æ®åº“æ¨¡å—
try:
    from db_helper import DBHelper, BookDB, multi_db_helper

    DB_AVAILABLE = True
except ImportError:
    print("âš ï¸ æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä»¥æµ‹è¯•æ¨¡å¼è¿è¡Œ")
    DB_AVAILABLE = False


class EdgeDoubanCrawler:
    """ä½¿ç”¨Edgeæµè§ˆå™¨çš„è±†ç“£å›¾ä¹¦çˆ¬è™«"""

    def __init__(self, db_helper=None, headless=True):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            db_helper: æ•°æ®åº“åŠ©æ‰‹å®ä¾‹
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        """
        # ä½¿ç”¨æä¾›çš„db_helperæˆ–åˆ›å»ºæ–°çš„
        if db_helper:
            self.db = db_helper
            self.book_db = BookDB(db_helper) if hasattr(db_helper, 'book_db') else None
        elif DB_AVAILABLE:
            # å¦‚æœæ²¡æœ‰æä¾›db_helperï¼Œä½†DB_AVAILABLEä¸ºTrueï¼Œä½¿ç”¨multi_db_helper
            self.db = multi_db_helper
            self.db.default_db = "book_db"  # è®¾ç½®ä¹¦åŸçš„é»˜è®¤æ•°æ®åº“
            self.book_db = BookDB(self.db)
        else:
            self.db = None
            self.book_db = None

        # åˆå§‹åŒ–Edgeæµè§ˆå™¨
        self.driver = self.init_edge_driver(headless=headless)

        # ç”¨æˆ·ä»£ç†
        self.user_agent = self.driver.execute_script("return navigator.userAgent;")
        print(f"ğŸ“± ä½¿ç”¨æµè§ˆå™¨: Edge - {self.user_agent[:50]}...")

        # å­˜å‚¨cookiesæ–‡ä»¶è·¯å¾„
        self.cookies_file = 'douban_cookies_edge.json'

    def init_edge_driver(self, headless=True):
        """åˆå§‹åŒ–Edgeæµè§ˆå™¨é©±åŠ¨"""
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–Edgeæµè§ˆå™¨...")

        # Edgeé€‰é¡¹
        edge_options = Options()

        if headless:
            edge_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            edge_options.add_argument('--disable-gpu')

        # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        edge_options.add_argument('--disable-blink-features=AutomationControlled')
        edge_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        edge_options.add_experimental_option('useAutomationExtension', False)

        # æ·»åŠ å…¶ä»–é€‰é¡¹
        edge_options.add_argument('--no-sandbox')
        edge_options.add_argument('--disable-dev-shm-usage')
        edge_options.add_argument('--disable-web-security')
        edge_options.add_argument('--allow-running-insecure-content')
        edge_options.add_argument('--window-size=1920,1080')

        # è®¾ç½®ç”¨æˆ·ä»£ç†
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.0.0',
        ]
        edge_options.add_argument(f'user-agent={random.choice(user_agents)}')

        # ç¦ç”¨å›¾ç‰‡åŠ è½½ä»¥åŠ å¿«é€Ÿåº¦ï¼ˆå¯é€‰ï¼‰
        # prefs = {"profile.managed_default_content_settings.images": 2}
        # edge_options.add_experimental_option("prefs", prefs)

        try:
            # Edgeé€šå¸¸ä¸éœ€è¦æŒ‡å®šé©±åŠ¨è·¯å¾„ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æŸ¥æ‰¾
            driver = webdriver.Edge(options=edge_options)

            # æ‰§è¡ŒJavaScriptæ¥éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            print("âœ… Edgeæµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            return driver

        except Exception as e:
            print(f"âŒ Edgeæµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print("1. ç¡®ä¿å·²å®‰è£…æœ€æ–°ç‰ˆMicrosoft Edgeæµè§ˆå™¨")
            print("2. å¯èƒ½éœ€è¦å®‰è£…Microsoft Edge WebDriver")
            print("3. ä¸‹è½½åœ°å€: https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/")
            raise

    def save_cookies(self):
        """ä¿å­˜cookiesåˆ°æ–‡ä»¶"""
        cookies = self.driver.get_cookies()
        with open(self.cookies_file, 'w', encoding='utf-8') as f:
            json.dump(cookies, f, ensure_ascii=False, indent=2)
        print(f"âœ… Cookieså·²ä¿å­˜åˆ° {self.cookies_file}")

    def load_cookies(self):
        """ä»æ–‡ä»¶åŠ è½½cookies"""
        if not os.path.exists(self.cookies_file):
            print(f"âš ï¸ Cookiesæ–‡ä»¶ä¸å­˜åœ¨: {self.cookies_file}")
            return False

        try:
            with open(self.cookies_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)

            # å…ˆè®¿é—®è±†ç“£é¦–é¡µä»¥è®¾ç½®åŸŸå
            self.driver.get('https://www.douban.com')
            time.sleep(2)

            # æ·»åŠ cookies
            for cookie in cookies:
                try:
                    # ç§»é™¤æ— æ•ˆå­—æ®µ
                    if 'expiry' in cookie:
                        cookie['expiry'] = int(cookie['expiry'])

                    self.driver.add_cookie(cookie)
                except Exception as e:
                    print(f"âš ï¸ æ·»åŠ cookieå¤±è´¥: {e}")
                    continue

            print(f"âœ… CookiesåŠ è½½æˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ åŠ è½½cookieså¤±è´¥: {e}")
            return False

    def login_douban(self, username=None, password=None):
        """æ‰‹åŠ¨ç™»å½•è±†ç“£"""
        print("ğŸ” æ­£åœ¨æ‰“å¼€è±†ç“£ç™»å½•é¡µé¢...")

        # æ‰“å¼€ç™»å½•é¡µé¢
        self.driver.get('https://www.douban.com/login')
        time.sleep(3)

        if username and password:
            print("å°è¯•è‡ªåŠ¨ç™»å½•...")
            try:
                # å°è¯•æŸ¥æ‰¾ç”¨æˆ·åå’Œå¯†ç è¾“å…¥æ¡†ï¼ˆè±†ç“£å¯èƒ½æœ‰å¤šç§ç™»å½•æ–¹å¼ï¼‰
                username_inputs = self.driver.find_elements(By.NAME, 'username')
                password_inputs = self.driver.find_elements(By.NAME, 'password')

                if username_inputs and password_inputs:
                    username_inputs[0].send_keys(username)
                    password_inputs[0].send_keys(password)

                    # æŸ¥æ‰¾ç™»å½•æŒ‰é’®
                    login_buttons = self.driver.find_elements(By.CLASS_NAME, 'btn-account')
                    if login_buttons:
                        login_buttons[0].click()
                    else:
                        # å°è¯•é€šè¿‡XPathæŸ¥æ‰¾
                        login_buttons = self.driver.find_elements(By.XPATH, "//input[@type='submit']")
                        if login_buttons:
                            login_buttons[0].click()

                    print("âœ… å·²æäº¤ç™»å½•ä¿¡æ¯")
                    time.sleep(3)
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°ç™»å½•è¡¨å•ï¼Œéœ€è¦æ‰‹åŠ¨ç™»å½•")

            except Exception as e:
                print(f"âŒ è‡ªåŠ¨ç™»å½•å¤±è´¥: {e}")
                print("è¯·æ‰‹åŠ¨ç™»å½•...")

        # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•
        input("ğŸ‘¤ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼ˆå¦‚æœ‰éªŒè¯ç è¯·å¤„ç†ï¼‰ï¼Œç„¶åæŒ‰å›è½¦é”®ç»§ç»­...")

        # ä¿å­˜cookies
        self.save_cookies()
        print("âœ… ç™»å½•å®Œæˆï¼Œcookieså·²ä¿å­˜")

    def search_books(self, keyword, count=10, scroll_times=3):
        """
        ä½¿ç”¨Edgeæœç´¢è±†ç“£ä¹¦ç±

        Args:
            keyword: æœç´¢å…³é”®è¯
            count: è¦è·å–çš„ä¹¦ç±æ•°é‡
            scroll_times: æ»šåŠ¨æ¬¡æ•°ä»¥åŠ è½½æ›´å¤šå†…å®¹
        """
        try:
            encoded_keyword = quote(keyword.encode('utf-8'))
            url = f'https://search.douban.com/book/subject_search?search_text={encoded_keyword}&cat=1001'

            print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
            print(f"ğŸŒ è®¿é—®URL: {url}")

            # è®¿é—®æœç´¢é¡µé¢
            self.driver.get(url)
            time.sleep(4)  # ç­‰å¾…åˆå§‹åŠ è½½

            # ç­‰å¾…é¡µé¢åŠ è½½
            print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")

            # æ–¹æ³•1ï¼šç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç°
            try:
                wait = WebDriverWait(self.driver, 20)
                # è±†ç“£æœç´¢ç»“æœé¡µé¢å¯èƒ½æœ‰å¤šç§ç»“æ„
                wait.until(
                    EC.presence_of_element_located(
                        (By.CSS_SELECTOR, "div.item-root, div.sc-bZQynM, div[data-id], div.title"))
                )
                print("âœ… é¡µé¢ä¸»è¦å†…å®¹å·²åŠ è½½")
            except Exception as e:
                print(f"âš ï¸ ç­‰å¾…è¶…æ—¶ï¼Œå¯èƒ½é¡µé¢ç»“æ„ä¸åŒ: {e}")
                # å³ä½¿è¶…æ—¶ä¹Ÿç»§ç»­ï¼Œå¯èƒ½é¡µé¢å·²ç»åŠ è½½äº†

            # æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼šéšæœºæ»šåŠ¨ä»¥åŠ è½½æ›´å¤šå†…å®¹
            print(f"ğŸ”„ æ¨¡æ‹Ÿæ»šåŠ¨ {scroll_times} æ¬¡ä»¥åŠ è½½æ›´å¤šå†…å®¹...")
            for i in range(scroll_times):
                # éšæœºæ»šåŠ¨
                scroll_height = random.randint(300, 1200)
                self.driver.execute_script(f"window.scrollTo(0, {scroll_height});")
                time.sleep(random.uniform(1.5, 3))

                # å¶å°”æ»šåŠ¨åˆ°åº•éƒ¨
                if i % 2 == 0:
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(random.uniform(2, 4))

            # æœ€åç­‰å¾…ä¸€ä¸‹è®©å†…å®¹å®Œå…¨åŠ è½½
            time.sleep(3)

            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source

            # ä¿å­˜é¡µé¢ç”¨äºè°ƒè¯•
            debug_file = f'edge_douban_{keyword}_{int(time.time())}.html'
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(page_source)
            print(f"ğŸ’¾ é¡µé¢æºç å·²ä¿å­˜åˆ° {debug_file}")

            # è§£æé¡µé¢
            soup = BeautifulSoup(page_source, 'html.parser')
            books = self.parse_search_results(soup, count)

            print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(books)} æœ¬ä¹¦")
            return books

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

            # ä¿å­˜é”™è¯¯æˆªå›¾
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                self.driver.save_screenshot(f'edge_error_{keyword}_{timestamp}.png')
                print(f"ğŸ“¸ é”™è¯¯æˆªå›¾å·²ä¿å­˜")
            except:
                pass

            return []

    def parse_search_results(self, soup, max_count):
        """è§£ææœç´¢ç»“æœé¡µé¢"""
        books = []

        print("ğŸ§  å¼€å§‹è§£æé¡µé¢å†…å®¹...")

        # é¦–å…ˆæ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰æ•°æ®
        if "æ­£åœ¨æœç´¢" in soup.text or "åŠ è½½ä¸­" in soup.text:
            print("âš ï¸ é¡µé¢å¯èƒ½è¿˜åœ¨åŠ è½½ä¸­ï¼Œæ•°æ®å¯èƒ½ä¸å®Œæ•´")

        # ç­–ç•¥1ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«ä¹¦ç±ä¿¡æ¯çš„div
        # è±†ç“£çš„ä¹¦ç±æ¡ç›®é€šå¸¸æœ‰è¿™äº›ç‰¹å¾
        book_candidates = []

        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"/subject/"çš„é“¾æ¥
        subject_links = soup.find_all('a', href=lambda x: x and '/subject/' in x)
        print(f"æ‰¾åˆ° {len(subject_links)} ä¸ªsubjecté“¾æ¥")

        # ä¸ºæ¯ä¸ªsubjecté“¾æ¥æ‰¾åˆ°æœ€è¿‘çš„çˆ¶divä½œä¸ºä¹¦ç±å®¹å™¨
        for link in subject_links[:max_count * 3]:  # å¤šæ‰¾ä¸€äº›
            # å‘ä¸Šæ‰¾çˆ¶å…ƒç´ ï¼Œç›´åˆ°æ‰¾åˆ°åˆé€‚çš„divå®¹å™¨
            parent = link.parent
            for _ in range(5):  # æœ€å¤šå‘ä¸Šæ‰¾5å±‚
                if parent and parent.name == 'div':
                    if parent not in book_candidates:
                        # æ£€æŸ¥è¿™ä¸ªdivæ˜¯å¦åŒ…å«ä¹¦ç±ç›¸å…³ä¿¡æ¯
                        has_title = link.text.strip()
                        has_other_info = parent.text and len(parent.text.strip()) > 20
                        if has_title and has_other_info:
                            book_candidates.append(parent)
                            break
                if parent:
                    parent = parent.parent
                else:
                    break

        # ç­–ç•¥2ï¼šæŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡ï¼Œæ‰¾åˆ°ä¹¦ç±å°é¢
        if len(book_candidates) < max_count:
            imgs = soup.find_all('img')
            for img in imgs:
                src = img.get('src', '')
                alt = img.get('alt', '')
                # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯ä¹¦ç±å°é¢
                if src and ('cover' in src or 'img' in src or 'book' in alt.lower()):
                    parent_div = img.find_parent('div')
                    if parent_div and parent_div not in book_candidates:
                        book_candidates.append(parent_div)

        print(f"æ€»å…±æ‰¾åˆ° {len(book_candidates)} ä¸ªå€™é€‰ä¹¦ç±æ¡ç›®")

        # æå–æ¯ä¸ªå€™é€‰æ¡ç›®çš„ä¿¡æ¯
        seen_titles = set()
        for i, item in enumerate(book_candidates):
            if len(books) >= max_count:
                break

            try:
                book_info = self.extract_book_info(item)
                if book_info and book_info.get('title'):
                    title = book_info['title']

                    # å»é‡
                    if title in seen_titles:
                        print(f"  âš ï¸ è·³è¿‡é‡å¤: {title[:30]}...")
                        continue

                    seen_titles.add(title)
                    books.append(book_info)
                    print(f"  âœ… {len(books)}. {title[:40]}... - {book_info.get('author', 'æœªçŸ¥')[:20]}")
            except Exception as e:
                print(f"  è§£æç¬¬{i + 1}ä¸ªæ¡ç›®æ—¶å‡ºé”™: {e}")
                continue

        return books

    def extract_book_info(self, item):
        """ä»å•ä¸ªæ¡ç›®ä¸­æå–ä¹¦ç±ä¿¡æ¯"""
        info = {}

        # 1. æå–æ ‡é¢˜ - æŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾ä¸­çš„æ–‡æœ¬
        title = ''

        # é¦–å…ˆæŸ¥æ‰¾åŒ…å«"/subject/"çš„é“¾æ¥æ–‡æœ¬
        for a in item.find_all('a', href=lambda x: x and '/subject/' in x):
            text = a.text.strip()
            if text and len(text) > 2 and len(text) < 100:
                title = text
                break

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾ä»»ä½•aæ ‡ç­¾
        if not title:
            for a in item.find_all('a'):
                text = a.text.strip()
                if text and 2 < len(text) < 100 and not text.startswith('http'):
                    title = text
                    break

        info['title'] = title

        # 2. æå–é“¾æ¥
        detail_url = ''
        for a in item.find_all('a', href=lambda x: x and '/subject/' in x):
            href = a.get('href', '')
            if '/subject/' in href:
                detail_url = href
                # ç¡®ä¿æ˜¯å®Œæ•´URL
                if not detail_url.startswith('http'):
                    detail_url = 'https://book.douban.com' + detail_url
                break

        info['detail_url'] = detail_url

        # 3. æå–å°é¢
        cover = ''
        for img in item.find_all('img'):
            src = img.get('src', '')
            if src and ('cover' in src or 'img' in src):
                cover = src
                # å°è¯•è·å–å¤§ä¸€ç‚¹çš„å›¾ç‰‡
                if 'spic' in cover:
                    cover = cover.replace('spic', 'lpic')
                elif 's_ratio' in cover:
                    cover = cover.replace('s_ratio', 'm_ratio')
                break

        info['cover'] = cover

        # 4. æå–ä½œè€…å’Œå‡ºç‰ˆç¤¾ä¿¡æ¯
        author = 'æœªçŸ¥ä½œè€…'
        publisher = ''

        # æŸ¥æ‰¾åŒ…å«æ–œæ åˆ†éš”çš„æ–‡æœ¬ï¼ˆä½œè€…/å‡ºç‰ˆç¤¾/å‡ºç‰ˆæ—¥æœŸ/ä»·æ ¼ï¼‰
        for elem in item.find_all(['div', 'span', 'p']):
            text = elem.text.strip()
            if '/' in text and 10 < len(text) < 200:
                parts = [p.strip() for p in text.split('/') if p.strip()]
                if len(parts) >= 3:
                    # é€šå¸¸æ ¼å¼ï¼šä½œè€… / å‡ºç‰ˆç¤¾ / å‡ºç‰ˆæ—¥æœŸ / ä»·æ ¼
                    author = parts[0]
                    if len(parts) >= 4:
                        # å°è¯•è¯†åˆ«å‡ºç‰ˆç¤¾ï¼ˆé€šå¸¸ä¸æ˜¯çº¯æ•°å­—ï¼‰
                        for part in parts[1:-2]:  # è·³è¿‡ç¬¬ä¸€ä¸ªï¼ˆä½œè€…ï¼‰å’Œæœ€åä¸¤ä¸ªï¼ˆæ—¥æœŸã€ä»·æ ¼ï¼‰
                            if not re.match(r'^\d', part) and len(part) < 30:
                                publisher = part
                                break
                    break

        info['author'] = author
        info['publisher'] = publisher

        # 5. æå–è¯„åˆ†
        rating = 4.0

        # æŸ¥æ‰¾è¯„åˆ†æ•°å­—ï¼ˆæ ¼å¼å¦‚ï¼š8.5ã€9.0ç­‰ï¼‰
        for elem in item.find_all(['span', 'div', 'p']):
            text = elem.text.strip()
            # åŒ¹é…æ•°å­—è¯„åˆ†
            rating_match = re.search(r'(\d+\.\d+)', text)
            if rating_match:
                try:
                    rating_val = float(rating_match.group(1))
                    if 1 <= rating_val <= 10:
                        rating = rating_val
                        break
                except:
                    pass

        info['rating'] = rating

        # 6. å…¶ä»–ä¿¡æ¯
        info['source'] = 'douban_edge'

        return info

    def get_book_detail(self, detail_url):
        """è·å–ä¹¦ç±è¯¦æƒ…é¡µä¿¡æ¯"""
        if not detail_url:
            return None

        print(f"ğŸ“– è·å–è¯¦æƒ…: {detail_url[:60]}...")

        try:
            # è®¿é—®è¯¦æƒ…é¡µ
            self.driver.get(detail_url)
            time.sleep(4)  # ç­‰å¾…é¡µé¢åŠ è½½

            # ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
            try:
                wait = WebDriverWait(self.driver, 15)
                # ç­‰å¾…ä¿¡æ¯åŒºåŸŸæˆ–æ ‡é¢˜åŠ è½½
                wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "#info, span[property='v:itemreviewed']"))
                )
            except:
                print("âš ï¸ è¯¦æƒ…é¡µåŠ è½½å¯èƒ½è¾ƒæ…¢æˆ–ç»“æ„ä¸åŒ")

            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')

            # è§£æè¯¦æƒ…ä¿¡æ¯
            detail = self.parse_detail_page(soup)

            print(f"  âœ… è¯¦æƒ…è·å–æˆåŠŸ")
            return detail

        except Exception as e:
            print(f"  âŒ è¯¦æƒ…è·å–å¤±è´¥: {e}")
            return None

    def parse_detail_page(self, soup):
        """è§£æè¯¦æƒ…é¡µä¿¡æ¯"""
        detail = {}

        # 1. æ ‡é¢˜
        title_elem = soup.find('span', property='v:itemreviewed')
        if title_elem:
            detail['title'] = title_elem.text.strip()
        else:
            # å¤‡é€‰é€‰æ‹©å™¨
            title_elem = soup.find('h1')
            if title_elem:
                detail['title'] = title_elem.text.strip()

        # 2. å°é¢ï¼ˆå¤§å›¾ï¼‰
        cover_elem = soup.find('img', alt=detail.get('title', ''))
        if not cover_elem:
            # å°è¯•å…¶ä»–é€‰æ‹©å™¨
            cover_elem = soup.find('a', class_='nbg')
            if cover_elem:
                cover_elem = cover_elem.find('img')

        if cover_elem and cover_elem.get('src'):
            cover_url = cover_elem['src']
            # æ›¿æ¢ä¸ºå¤§å°ºå¯¸
            if 's_ratio_poster' in cover_url:
                cover_url = cover_url.replace('s_ratio_poster', 'l_ratio_poster')
            detail['cover'] = cover_url

        # 3. ä¿¡æ¯åŒºåŸŸï¼ˆä½œè€…ã€å‡ºç‰ˆç¤¾ç­‰ï¼‰
        info_soup = soup.find('div', id='info')
        if info_soup:
            info_text = info_soup.get_text('\n', strip=True)

            # æå–ä½œè€…
            author_match = re.search(r'ä½œè€…[:\s]\s*(.+)', info_text)
            if author_match:
                detail['author'] = author_match.group(1).split('\n')[0].strip()

            # æå–å‡ºç‰ˆç¤¾
            publisher_match = re.search(r'å‡ºç‰ˆç¤¾[:\s]\s*(.+)', info_text)
            if publisher_match:
                detail['publisher'] = publisher_match.group(1).split('\n')[0].strip()

            # æå–å‡ºç‰ˆæ—¥æœŸ
            pubdate_match = re.search(r'å‡ºç‰ˆå¹´[:\s]\s*(.+)', info_text)
            if pubdate_match:
                detail['publish_date'] = pubdate_match.group(1).split('\n')[0].strip()

            # æå–ISBN
            isbn_match = re.search(r'ISBN[:\s]\s*(\d+)', info_text)
            if isbn_match:
                detail['isbn'] = isbn_match.group(1).strip()

            # æå–é¡µæ•°
            pages_match = re.search(r'é¡µæ•°[:\s]\s*(\d+)', info_text)
            if pages_match:
                try:
                    detail['pages'] = int(pages_match.group(1))
                except:
                    detail['pages'] = 0

        # 4. è¯„åˆ†
        rating_elem = soup.find('strong', class_='ll rating_num')
        if rating_elem:
            try:
                detail['rating'] = float(rating_elem.text.strip())
            except:
                detail['rating'] = 4.0

        # 5. ç®€ä»‹
        intro_elem = soup.find('div', class_='intro')
        if intro_elem:
            # è·å–æ‰€æœ‰æ®µè½
            paragraphs = intro_elem.find_all('p')
            brief = ' '.join([p.text.strip() for p in paragraphs if p.text.strip()])
            if brief:
                detail['brief'] = brief[:300] + '...' if len(brief) > 300 else brief

        # 6. æ ‡ç­¾
        tags_elem = soup.find('div', id='db-tags-section')
        if tags_elem:
            tags = tags_elem.find_all('a', class_='tag')
            tag_list = [tag.text.strip() for tag in tags[:5]]
            detail['tags'] = tag_list

        return detail

    def crawl_and_save(self, keywords, books_per_keyword=5, use_details=True):
        """çˆ¬å–å¹¶ä¿å­˜ä¹¦ç±ä¿¡æ¯ - ä¸æ”¹åŠ¨BookDBçš„å»é‡ç‰ˆæœ¬"""
        all_books = []

        for keyword in keywords:
            print(f"\n{'=' * 60}")
            print(f"å¼€å§‹çˆ¬å–: {keyword}")
            print(f"{'=' * 60}")

            # æœç´¢ä¹¦ç±ï¼Œå¤šå–ä¸€ç‚¹å¤‡ç”¨ï¼ˆé˜²æ­¢é‡å¤å¯¼è‡´æŠ“å–æ•°é‡ä¸è¶³ï¼‰
            books = self.search_books(keyword, count=books_per_keyword * 3)

            if not books:
                print(f"âŒ å…³é”®è¯ '{keyword}' æœªæ‰¾åˆ°ä»»ä½•ä¹¦ç±")
                continue

            # ä½¿ç”¨è®¡æ•°å™¨ç¡®ä¿æ¯ä¸ªå…³é”®è¯æŠ“å–åˆ°è¶³å¤Ÿé‡çš„æ–°ä¹¦
            processed_count = 0

            for book in books:
                if processed_count >= books_per_keyword:
                    break

                # --- æ ¸å¿ƒä¿®æ”¹ï¼šåœ¨å¤„ç†é€»è¾‘æœ€å¼€å§‹æå– title ---
                title = book.get('title', 'æœªçŸ¥')

                # --- å…³é”®ï¼šç›´æ¥è°ƒç”¨ db_helper çš„åº•å±‚ query æ–¹æ³•è¿›è¡Œå»é‡æ£€æŸ¥ ---
                # è¿™ç§æ–¹æ³•ä¸éœ€è¦åœ¨ BookDB é‡Œå†™æ–°å‡½æ•°ï¼Œç›´æ¥åœ¨çˆ¬è™«é‡Œå†™ SQL
                if self.db and DB_AVAILABLE:
                    try:
                        # è¿™é‡Œçš„ self.db æŒ‡å‘çš„æ˜¯ä½ çš„ DBHelper å®ä¾‹
                        check_sql = "SELECT id FROM books WHERE title = %s LIMIT 1"
                        # æ‰§è¡ŒæŸ¥è¯¢ï¼Œæ³¨æ„ params å¿…é¡»æ˜¯å…ƒç»„å½¢å¼ (%s, )
                        exists = self.db.query(check_sql, (title,))

                        if exists:
                            print(f"  >> [æ•°æ®åº“å·²å­˜åœ¨] è·³è¿‡: ã€Š{title}ã€‹")
                            continue
                    except Exception as e:
                        print(f"  âš ï¸ æŸ¥é‡å¤±è´¥ (è·³è¿‡æ£€æŸ¥): {e}")

                print(f"\n[{processed_count + 1}/{books_per_keyword}] å¤„ç†: {title}")

                # è·å–è¯¦æƒ…ï¼ˆå¯é€‰ï¼‰
                detail = None
                if use_details and book.get('detail_url'):
                    detail = self.get_book_detail(book['detail_url'])

                if detail:
                    # åˆå¹¶ä¿¡æ¯ï¼Œè¯¦æƒ…é¡µä¿¡æ¯ä¼˜å…ˆ
                    book_info = {**book, **detail}
                else:
                    book_info = book

                # è¡¥å……ç¼ºå¤±å­—æ®µ - æ·»åŠ åç«¯éœ€è¦çš„å­—æ®µ
                if 'brief' not in book_info or not book_info['brief']:
                    book_info['brief'] = f'{keyword}ç›¸å…³ä¹¦ç±ï¼Œå†…å®¹ç²¾å½©...'

                book_info.setdefault('category', self.map_category(keyword))
                book_info.setdefault('pages', random.randint(200, 400))
                book_info.setdefault('rating', 4.0 + random.random() * 2)

                # å¤„ç†æ ‡ç­¾
                if 'tags' in book_info and isinstance(book_info['tags'], list):
                    tags_str = ','.join(book_info['tags'][:3])
                else:
                    tags_str = keyword

                book_info['tags'] = tags_str

                # æ·»åŠ åç«¯éœ€è¦çš„é¢å¤–å­—æ®µ
                book_info.setdefault('content', '')
                book_info.setdefault('chapters', [])
                book_info.setdefault('publisher', book_info.get('publisher', ''))
                book_info.setdefault('publish_date', book_info.get('publish_date', ''))
                book_info.setdefault('isbn', book_info.get('isbn', ''))
                book_info.setdefault('source', 'douban_edge')

                # ä¿å­˜åˆ°æ•°æ®åº“
                if self.db and DB_AVAILABLE:
                    try:
                        book_id = self.save_to_database(book_info)
                        if book_id:
                            all_books.append(book_info)
                            processed_count += 1
                            print(f"  âœ… ä¿å­˜æˆåŠŸ (ID: {book_id})")
                        else:
                            print(f"  âš ï¸ æ•°æ®åº“ä¿å­˜å¤±è´¥æˆ–ä¹¦ç±å·²é‡å¤")
                    except Exception as e:
                        print(f"  âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
                else:
                    processed_count += 1
                    print(f"  âš ï¸ æ•°æ®åº“ä¸å¯ç”¨ï¼Œè·³è¿‡ä¿å­˜")

                # éšæœºå»¶è¿Ÿ
                delay = random.uniform(2, 5)
                print(f"  â³ ç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)

        print(f"\n{'=' * 60}")
        print(f"âœ… ä»»åŠ¡å®Œæˆï¼æœ¬æ¬¡å…±å…¥åº“ {len(all_books)} æœ¬æ–°ä¹¦")
        print(f"{'=' * 60}")

        return all_books

    def map_category(self, keyword):
        """æ˜ å°„å…³é”®è¯åˆ°åˆ†ç±»"""
        category_map = {
            'å¿ƒç†å­¦': 'å¿ƒç†å…¥é—¨',
            'å¿ƒç†': 'å¿ƒç†å…¥é—¨',
            'æƒ…ç»ª': 'å¿ƒç†å…¥é—¨',
            'æ­£å¿µ': 'æ­£å¿µå†¥æƒ³',
            'å†¥æƒ³': 'æ­£å¿µå†¥æƒ³',
            'å‹åŠ›': 'å‹åŠ›ç®¡ç†',
            'ç„¦è™‘': 'å‹åŠ›ç®¡ç†',
            'è‡ªæˆ‘æˆé•¿': 'è‡ªæˆ‘æˆé•¿',
            'ä¸ªäººæˆé•¿': 'è‡ªæˆ‘æˆé•¿',
            'æˆåŠŸå­¦': 'è‡ªæˆ‘æˆé•¿',
            'æƒ…å•†': 'å¿ƒç†å…¥é—¨',
            'å¿ƒç†æ²»ç–—': 'å¿ƒç†å…¥é—¨',
            'å¿ƒç†å’¨è¯¢': 'å¿ƒç†å…¥é—¨'
        }

        for k, v in category_map.items():
            if k in keyword:
                return v

        return 'å¿ƒç†å…¥é—¨'  # é»˜è®¤åˆ†ç±»

    def save_to_database(self, book_info):
        """ä¿å­˜ä¹¦ç±ä¿¡æ¯åˆ°æ•°æ®åº“ - ä¿®æ”¹ç‰ˆï¼Œä½¿ç”¨db_helperçš„æ–¹æ³•"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé€šè¿‡æ ‡é¢˜å’Œä½œè€…ï¼‰
            check_sql = """
                SELECT id FROM books 
                WHERE title = %s AND author LIKE %s 
                LIMIT 1
            """

            # ä½¿ç”¨db_helperçš„queryæ–¹æ³•
            result = self.db.query(
                check_sql,
                (book_info['title'], f"%{book_info.get('author', '')}%")
            )

            if result:
                print(f"  âš ï¸ ä¹¦ç±å·²å­˜åœ¨: {book_info['title']}")
                return result[0]['id']

            # æ’å…¥æ–°ä¹¦ç± - ä¸ä½ çš„è¡¨ç»“æ„å¯¹é½
            insert_sql = """
                INSERT INTO books 
                (title, author, cover, brief, category, tags, content, chapters,
                 rating, pages, publisher, publish_date, isbn, source, created_at, updated_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
            """

            # å¤„ç†contentå’Œchapterså­—æ®µï¼ˆå¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨ç©ºå€¼ï¼‰
            content = book_info.get('content', '')
            chapters = book_info.get('chapters', [])

            # å¦‚æœchaptersæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            if isinstance(chapters, list):
                chapters = json.dumps(chapters, ensure_ascii=False)

            params = (
                book_info.get('title', ''),
                book_info.get('author', 'æœªçŸ¥ä½œè€…'),
                book_info.get('cover', ''),
                book_info.get('brief', ''),
                book_info.get('category', 'å¿ƒç†å…¥é—¨'),
                book_info.get('tags', ''),
                content,  # contentå­—æ®µ
                chapters,  # chapterså­—æ®µï¼ˆJSONæ ¼å¼ï¼‰
                float(book_info.get('rating', 4.5)),
                int(book_info.get('pages', 0)),
                book_info.get('publisher', ''),
                book_info.get('publish_date', None),
                book_info.get('isbn', ''),
                book_info.get('source', 'douban_edge')
            )

            # æ‰§è¡Œæ’å…¥ï¼Œä½¿ç”¨insertæ–¹æ³•ï¼ˆè¿”å›å½±å“è¡Œæ•°ï¼‰
            result = self.db.insert(insert_sql, params)

            # å¦‚æœæ’å…¥æˆåŠŸï¼Œè·å–æœ€åæ’å…¥çš„ID
            if result > 0:
                # æŸ¥è¯¢æœ€åæ’å…¥çš„ID
                last_id_sql = "SELECT LAST_INSERT_ID() as id"
                id_result = self.db.query(last_id_sql)
                if id_result:
                    book_id = id_result[0]['id']
                    return book_id

            print(f"  æ’å…¥å¤±è´¥ï¼Œå½±å“è¡Œæ•°: {result}")
            return None

        except Exception as e:
            print(f"  æ•°æ®åº“ä¿å­˜é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if hasattr(self, 'driver') and self.driver:
            print("ğŸ‘‹ æ­£åœ¨å…³é—­Edgeæµè§ˆå™¨...")
            self.driver.quit()
            print("âœ… Edgeæµè§ˆå™¨å·²å…³é—­")


def quick_test():
    """å¿«é€Ÿæµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª Edgeè±†ç“£çˆ¬è™«å¿«é€Ÿæµ‹è¯•")

    crawler = EdgeDoubanCrawler(headless=False)  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£ä»¥ä¾¿è§‚å¯Ÿ

    try:
        # æµ‹è¯•æœç´¢
        books = crawler.search_books('å¿ƒç†å­¦', count=3)

        if books:
            print(f"\nâœ… æˆåŠŸæ‰¾åˆ° {len(books)} æœ¬ä¹¦:")
            for i, book in enumerate(books):
                print(f"\n{i + 1}. ã€Š{book.get('title')}ã€‹")
                print(f"   ä½œè€…: {book.get('author')}")
                print(f"   è¯„åˆ†: {book.get('rating')}")
                print(f"   å°é¢: {book.get('cover', 'æ— ')[:50]}...")
                if book.get('detail_url'):
                    print(f"   é“¾æ¥: {book.get('detail_url')[:80]}...")
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•ä¹¦ç±")

            # æ˜¾ç¤ºå½“å‰é¡µé¢
            input("\næŒ‰å›è½¦é”®æŸ¥çœ‹æµè§ˆå™¨é¡µé¢...")
            print("æµè§ˆå™¨çª—å£åº”æ˜¾ç¤ºè±†ç“£æœç´¢é¡µé¢")

    finally:
        crawler.close()


def main():
    """ä¸»å‡½æ•° - ä¿®æ”¹ç‰ˆ"""
    print("=" * 60)
    print("ğŸ“š Edgeæµè§ˆå™¨è±†ç“£ä¹¦ç±çˆ¬è™«")
    print("=" * 60)

    print("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. å¿«é€Ÿæµ‹è¯•ï¼ˆä¸ä¿å­˜æ•°æ®ï¼‰")
    print("2. å®Œæ•´çˆ¬å–ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰")
    print("3. ç™»å½•è±†ç“£å¹¶ä¿å­˜cookies")
    print("4. ä»…æœç´¢å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶")

    mode = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3/4): ").strip()

    if mode == '1':
        # å¿«é€Ÿæµ‹è¯•
        quick_test()

    elif mode == '2':
        # å®Œæ•´çˆ¬å–æ¨¡å¼
        if not DB_AVAILABLE:
            print("âŒ æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥db_helper.py")
            return

        try:
            # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼Œä¼ å…¥æ•°æ®åº“è¿æ¥
            crawler = EdgeDoubanCrawler(headless=True)

            try:
                # è·å–çˆ¬å–å…³é”®è¯
                keywords_input = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰: ").strip()
                if keywords_input:
                    keywords = [k.strip() for k in keywords_input.split(',') if k.strip()]
                else:
                    keywords = ['å¿ƒç†å­¦', 'æ­£å¿µå†¥æƒ³', 'å‹åŠ›ç®¡ç†']
                    print(f"ä½¿ç”¨é»˜è®¤å…³é”®è¯: {keywords}")

                # è·å–çˆ¬å–æ•°é‡
                try:
                    count = int(input("æ¯ä¸ªå…³é”®è¯çˆ¬å–å‡ æœ¬ä¹¦? (é»˜è®¤3): ") or "3")
                except:
                    count = 3

                # æ˜¯å¦è·å–è¯¦æƒ…
                use_details_input = input("æ˜¯å¦è·å–æ¯æœ¬ä¹¦çš„è¯¦æƒ…é¡µä¿¡æ¯? (y/n, é»˜è®¤y): ").strip().lower()
                use_details = use_details_input != 'n'

                # å¼€å§‹çˆ¬å–
                books = crawler.crawl_and_save(keywords, books_per_keyword=count, use_details=use_details)

                # ç»Ÿè®¡ç»“æœ
                if crawler.db:
                    count_sql = "SELECT COUNT(*) as total FROM books"
                    result = crawler.db.query(count_sql)
                    print(f"\nğŸ“Š æ•°æ®åº“ç°æœ‰ä¹¦ç±æ€»æ•°: {result[0]['total']} æœ¬")

                    # æ˜¾ç¤ºæœ€è¿‘æ·»åŠ çš„ä¹¦ç±
                    recent_sql = "SELECT id, title, author FROM books ORDER BY id DESC LIMIT 5"
                    recent_books = crawler.db.query(recent_sql)
                    print("\nğŸ“š æœ€è¿‘æ·»åŠ çš„ä¹¦ç±:")
                    for book in recent_books:
                        print(f"  ID:{book['id']} - ã€Š{book['title']}ã€‹ - {book['author']}")

            finally:
                crawler.close()

        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥æˆ–çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    elif mode == '3':
        # ç™»å½•æ¨¡å¼
        print("\nğŸ” è±†ç“£ç™»å½•æ¨¡å¼")

        crawler = EdgeDoubanCrawler(headless=False)  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£

        try:
            username = input("è±†ç“£ç”¨æˆ·å/é‚®ç®± (å¯é€‰ï¼Œç›´æ¥å›è½¦è·³è¿‡è‡ªåŠ¨ç™»å½•): ").strip()
            password = input("è±†ç“£å¯†ç  (å¯é€‰): ").strip() if username else None

            crawler.login_douban(username if username else None, password)

            print("\nâœ… ç™»å½•å®Œæˆï¼")
            print("ä¸‹æ¬¡è¿è¡Œçˆ¬è™«æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨ä¿å­˜çš„cookies")

        finally:
            crawler.close()

    elif mode == '4':
        # ä»…æœç´¢å¹¶ä¿å­˜ä¸ºJSON
        crawler = EdgeDoubanCrawler(headless=True)

        try:
            # è·å–çˆ¬å–å…³é”®è¯
            keywords_input = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰: ").strip()
            if keywords_input:
                keywords = [k.strip() for k in keywords_input.split(',') if k.strip()]
            else:
                keywords = ['å¿ƒç†å­¦', 'è‡ªæˆ‘æˆé•¿']
                print(f"ä½¿ç”¨é»˜è®¤å…³é”®è¯: {keywords}")

            # è·å–çˆ¬å–æ•°é‡
            try:
                count = int(input("æ¯ä¸ªå…³é”®è¯çˆ¬å–å‡ æœ¬ä¹¦? (é»˜è®¤5): ") or "5")
            except:
                count = 5

            # å¼€å§‹çˆ¬å–
            books = crawler.crawl_and_save(keywords, books_per_keyword=count, use_details=False)

            if books:
                # ä¿å­˜ä¸ºJSONæ–‡ä»¶
                filename = f'douban_books_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(books, f, ensure_ascii=False, indent=2)
                print(f"âœ… æˆåŠŸæ”¶é›† {len(books)} æœ¬ä¹¦ç±æ•°æ®")
                print(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")

        finally:
            crawler.close()

    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œé€€å‡ºç¨‹åº")


if __name__ == '__main__':
    main()